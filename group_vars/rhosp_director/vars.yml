rhosp_director:
  poweroff: true
  bootdev: network
  validation_errors: []
  
  # stack user gets created and will be used for all RHOSP actions on undercloud
  stack_user_pwd: "{{ vault_stack_user_pwd }}"

  # All variales for undercloud.conf
  cloud_domain: "{{ domain }}"
  hostname_short: undercloud
  ntp_servers: time.google.com #TODO: public or local NTP? maybe KVM or Cisco Switch
  # Network interface on the Undercloud that will be handling the PXE
  # boots and DHCP for Overcloud instances
  provisioning_interface: eth0
  # IP information for the interface on the Undercloud that will be
  # handling the PXE boots and DHCP for Overcloud instances.  The IP
  # portion of the value will be assigned to the network interface
  # defined by local_interface, with the netmask defined by the prefix
  # portion of the value
  provisioning_ip: 172.16.20.16/24
  # Network CIDR for the Neutron-managed network for Overcloud
  # instances. This should be the subnet used for PXE booting
  provisioning_network_cidr: 172.16.20.0/24
  # Network gateway for the Neutron-managed network for Overcloud
  # instances. This should match the local_ip above when using masquerading
  provisioning_network_gateway: 172.16.20.1
  # Virtual IP address to use for the admin endpoints of Undercloud services.
  admin_apis_vip: 172.16.20.17
  # Temporary IP range that will be given to nodes during the discovery
  # process.  Should not overlap with the range defined by dhcp_start
  # and dhcp_end, but should be in the same network
  inspection_dhcp_start: 172.16.20.20
  inspection_dhcp_end: 172.16.20.39
  # Start of DHCP allocation range for PXE and DHCP of Overcloud instances
  deploy_dhcp_start: 172.16.20.40
  # End of DHCP allocation range for PXE and DHCP of Overcloud instances
  deploy_dhcp_end: 172.16.20.69
  # Defines whether to wipe the hard drive of overcloud nodes between
  # deployments and after the introspection
  clean_nodes: false
  # Set admin password for undercloud
  admin_password: "{{ vault_undercloud_admin_pwd }}"

  overcloud_nodes:
    - name: node1
      profile: control
      pm_addr: "172.16.20.11"
      pm_user: "admin"
      pm_pwd: "admin"
      #NOTE: If pxe_ssh, ipmi_addr should be IP of hypervisor. If pxe_ipmitool,
      #      should be a list of IPs to check and register
      pm_driver: "pxe_ipmitool"
    - name: node2
      profile: compute
      pm_addr: "172.16.20.12"
      pm_user: "admin"
      pm_pwd: "admin"
      #NOTE: If pxe_ssh, ipmi_addr should be IP of hypervisor. If pxe_ipmitool,
      #      should be a list of IPs to check and register
      pm_driver: "pxe_ipmitool"
    - name: node3
      profile: compute
      pm_addr: "172.16.20.13"
      pm_user: "admin"
      pm_pwd: "admin"
      #NOTE: If pxe_ssh, ipmi_addr should be IP of hypervisor. If pxe_ipmitool,
      #      should be a list of IPs to check and register
      pm_driver: "pxe_ipmitool"
    - name: node4
      profile: compute
      pm_addr: "172.16.20.14"
      pm_user: "admin"
      pm_pwd: "admin"
      #NOTE: If pxe_ssh, ipmi_addr should be IP of hypervisor. If pxe_ipmitool,
      #      should be a list of IPs to check and register
      pm_driver: "pxe_ipmitool"
